{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lowercasing"
      ],
      "metadata": {
        "id": "KgM-PS3FtQa9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48ahVHmItKTz",
        "outputId": "c70db98d-f4e0-405b-f934-5f685d0d84cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercasing: text preprocessing: converting all text to lowercase.\n"
          ]
        }
      ],
      "source": [
        "text_lowercasing = \"Text Preprocessing: CONVERTING ALL TEXT TO LOWERCASE.\"\n",
        "lowercased_text = text_lowercasing.lower()\n",
        "print(\"Lowercasing:\", lowercased_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Punctuations"
      ],
      "metadata": {
        "id": "h-_AvTAmteY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text_punctuation = \"Text preprocessing involves removing punctuation, which can be noisy!!!\"\n",
        "\n",
        "# Removing Punctuation\n",
        "no_punctuation_text = re.sub(r'[^\\w\\s]', '', text_punctuation) #^\\w matches any non-word character. ^\\s matches any non-whitespace character.\n",
        "print(\"Removing Punctuation:\", no_punctuation_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg30tmMGtazv",
        "outputId": "e80bf15e-38c4-4c94-f487-4ce848af04c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing Punctuation: Text preprocessing involves removing punctuation which can be noisy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Words"
      ],
      "metadata": {
        "id": "b-WyG41o2pmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "text = \"Text preprocessing is an important step in Natural Language Processing!\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_text = [word for word in nltk.word_tokenize(text) if word.lower() not in stop_words]\n",
        "print(\"Removing Stop Words:\", filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_nuAvFduAo9",
        "outputId": "5285e6a3-fefc-490c-9045-5ca2b2a79fda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing Stop Words: ['Text', 'preprocessing', 'important', 'step', 'Natural', 'Language', 'Processing', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Characters"
      ],
      "metadata": {
        "id": "g5-sBl4e2slW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text_special_characters = \"Text preprocessing removes special characters, such as @#$%^&*(), and numbers 123.\"\n",
        "\n",
        "no_special_characters_text = re.sub('[^A-Za-z ]+', '', text_special_characters)\n",
        "print(\"Removing Special Characters and Numbers:\", no_special_characters_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaT0cVpvuNAF",
        "outputId": "61e61348-b001-4a97-af4e-f1a42d522a9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing Special Characters and Numbers: Text preprocessing removes special characters such as  and numbers \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "2-597oqM2vnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Sample text for demonstration\n",
        "text_stemming = \"Text preprocessing involves stemmer, stemming, stemmed words.\"\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_text = ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(text_stemming)])\n",
        "print(\"Stemming:\", stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyep9coRvY52",
        "outputId": "9d026c25-7907-4712-b7e3-d8350f894a12"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming: text preprocess involv stemmer , stem , stem word .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "9COIwXPq2yZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to get WordNet POS tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif tag.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif tag.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return 'n'  # Noun (default)\n",
        "\n",
        "# Sample text for demonstration\n",
        "text = \"Lemmatization is a more advanced text preprocessing technique than stemming.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform part-of-speech tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Lemmatization with part-of-speech tags\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "\n",
        "print(\"Lemmatization:\", ' '.join(lemmatized_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAVr45-cvxZQ",
        "outputId": "7c5eddd5-efbf-4b7f-d6f8-c10f7e02b625"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization: Lemmatization be a more advanced text preprocessing technique than stem .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## URL remover"
      ],
      "metadata": {
        "id": "nTkaOHDK21it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "html_text = \"<p>This is an example <b>HTML</b> text with <a href='http://example.com'>URL</a>.</p>\"\n",
        "\n",
        "# Removing HTML Tags\n",
        "no_html_text = re.sub('<[^<]+?>', '', html_text)\n",
        "print(\"Removing HTML Tags:\", no_html_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecfhiOsAwDCK",
        "outputId": "9204e12c-e995-4025-f17f-4c8da79edd1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing HTML Tags: This is an example HTML text with URL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Abbreviations"
      ],
      "metadata": {
        "id": "UtmClAcA24WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_abbreviations = \"Dr. Smith and Mr. Johnson are attending the conference.\"\n",
        "\n",
        "# Handling Abbreviations\n",
        "abbreviation_mapping = {\"Dr.\": \"Doctor\", \"Mr.\": \"Mister\"}\n",
        "for abbreviation, expanded in abbreviation_mapping.items():\n",
        "    text_abbreviations = text_abbreviations.replace(abbreviation, expanded)\n",
        "print(\"Handling Abbreviations:\", text_abbreviations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TXTk7cKxtJ7",
        "outputId": "225f1bc8-0177-4a73-ba5d-e2d1d12fb3ab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handling Abbreviations: Doctor Smith and Mister Johnson are attending the conference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spell Checking"
      ],
      "metadata": {
        "id": "7q2BavAx28nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "text_spell_checking = \"This is an exampel sentence with misspellled words.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = text_spell_checking.split()\n",
        "\n",
        "# Spell Checking\n",
        "spell = SpellChecker()\n",
        "corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in tokens]\n",
        "corrected_text = ' '.join(corrected_words)\n",
        "print(\"Spell Checking:\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcyZBJuMx2vP",
        "outputId": "203c8359-3201-487c-ffea-ef2a874fefa6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Spell Checking: This is an example sentence with misspelled words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Rare Words\n"
      ],
      "metadata": {
        "id": "LOP20BYz3c8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text for demonstration\n",
        "text_rare_words = \"This is a sample text with some repeated and rare words. Sample text.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text_rare_words)\n",
        "\n",
        "# Removing Rare Words\n",
        "word_frequency = Counter(tokens)\n",
        "rare_words = [word for word, count in word_frequency.items() if count == 1]\n",
        "\n",
        "print(\"Rare Words:\", rare_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnnPQ9SEyB1o",
        "outputId": "b86dc649-2583-4149-b18b-085b990914bc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rare Words: ['This', 'is', 'a', 'sample', 'with', 'some', 'repeated', 'and', 'rare', 'words', 'Sample']\n"
          ]
        }
      ]
    }
  ]
}